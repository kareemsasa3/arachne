services:
  # Certbot service for automatic SSL certificate management
  certbot:
    image: certbot/certbot:latest
    container_name: arachne-certbot
    volumes:
      - ./nginx/ssl:/etc/letsencrypt
      - ./nginx/certbot/www:/var/www/certbot
      - ./logs/certbot:/var/log/letsencrypt
    command: certonly --webroot --webroot-path=/var/www/certbot --email ${SSL_EMAIL} --agree-tos --no-eff-email --force-renewal -d ${DOMAIN_NAME}
    profiles:
      - ssl-setup
    depends_on:
      - nginx
    restart: "no"

  # Nginx reverse proxy - Production optimized
  nginx:
    image: nginx:alpine
    container_name: arachne-nginx-prod
    environment:
      - DOMAIN_NAME=${DOMAIN_NAME}
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/letsencrypt:ro
      - ./nginx/certbot/www:/var/www/certbot:ro
      - ./logs/nginx:/var/log/nginx
      - ./nginx/conf.d/default.conf.template:/tmp/default.conf.template:ro
    depends_on:
      ai:
        condition: service_healthy
      scraper:
        condition: service_healthy
    restart: unless-stopped
    command: >
      /bin/sh -c "sed 's/__DOMAIN_NAME__/'\"$DOMAIN_NAME\"'/g' /tmp/default.conf.template > /tmp/default.conf && nginx -g 'daemon off;'"
    deploy:
      resources:
        limits:
          memory: ${NGINX_MEMORY_LIMIT:-256M}
          cpus: "${NGINX_CPU_LIMIT:-0.5}"
        reservations:
          memory: 128M
          cpus: "0.25"

  # AI service - Production optimized from GHCR
  ai:
    image: ${AI_IMAGE:-ghcr.io/kareemsasa3/personal-website/ai:latest}
    container_name: arachne-ai-prod
    environment:
      - NODE_ENV=${AI_NODE_ENV:-production}
      - PORT=${AI_PORT:-3001}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - LOG_LEVEL=${AI_LOG_LEVEL:-info}
      - ENABLE_METRICS=${AI_ENABLE_METRICS:-true}
      - RATE_LIMIT_WINDOW_MS=${AI_RATE_LIMIT_WINDOW_MS:-900000}
      - RATE_LIMIT_MAX_REQUESTS=${AI_RATE_LIMIT_MAX_REQUESTS:-100}
      - SCRAPER_URL=${SCRAPER_URL:-http://scraper:8080}
      - SCRAPER_API_TOKEN=${SCRAPER_API_TOKEN}
      - PROFILE_CONTEXT_PATH=/app/profile/profile.md
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:3001/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: ${AI_MEMORY_LIMIT:-1G}
          cpus: "${AI_CPU_LIMIT:-1.0}"
        reservations:
          memory: 512M
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Scraper - Production optimized from GHCR
  scraper:
    image: ${SCRAPER_IMAGE:-ghcr.io/kareemsasa3/personal-website/scraper:latest}
    container_name: arachne-scraper-prod
    environment:
      - SCRAPER_REDIS_ADDR=${SCRAPER_REDIS_ADDR:-redis:6379}
      - SCRAPER_REDIS_DB=${SCRAPER_REDIS_DB:-0}
      - SCRAPER_ENABLE_METRICS=${SCRAPER_ENABLE_METRICS:-true}
      - SCRAPER_ENABLE_LOGGING=${SCRAPER_ENABLE_LOGGING:-true}
      - SCRAPER_LOG_LEVEL=${SCRAPER_LOG_LEVEL:-info}
      - SCRAPER_MAX_CONCURRENT=${SCRAPER_MAX_CONCURRENT:-10}
      - SCRAPER_REQUEST_TIMEOUT=${SCRAPER_REQUEST_TIMEOUT:-120s}
      - SCRAPER_TOTAL_TIMEOUT=${SCRAPER_TOTAL_TIMEOUT:-180s}
      - SCRAPER_USE_HEADLESS=${SCRAPER_USE_HEADLESS:-true}
      - SCRAPER_USER_AGENT=${SCRAPER_USER_AGENT:-Mozilla/5.0 (compatible; ArachneBot/1.0)}
      - SCRAPER_RATE_LIMIT=${SCRAPER_RATE_LIMIT:-2}
      - SCRAPER_RATE_LIMIT_WINDOW=${SCRAPER_RATE_LIMIT_WINDOW:-1s}
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:8080/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: ${SCRAPER_MEMORY_LIMIT:-2G}
          cpus: "0.8"
        reservations:
          memory: 1G
          cpus: "0.4"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis - Production optimized
  redis:
    image: redis:7-alpine
    container_name: arachne-redis-prod
    volumes:
      - redis_data_prod:/data
      - ./logs/redis:/var/log/redis
    command: >
      redis-server 
      --appendonly yes 
      --maxmemory ${REDIS_MAX_MEMORY:-512mb} 
      --maxmemory-policy ${REDIS_MAX_MEMORY_POLICY:-allkeys-lru}
      --save 900 1
      --save 300 10
      --save 60 10000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_LIMIT:-1G}
          cpus: "${REDIS_CPU_LIMIT:-1.0}"
        reservations:
          memory: 512M
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis Commander - Optional, can be disabled in production
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: arachne-redis-commander-prod
    environment:
      - REDIS_HOSTS=local:redis:${REDIS_PORT:-6379}
    depends_on:
      - redis
    restart: unless-stopped
    profiles:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.25"

volumes:
  redis_data_prod:
    driver: local
