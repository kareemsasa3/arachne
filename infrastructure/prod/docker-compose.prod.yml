services:
  # Certbot service for automatic SSL certificate management
  certbot:
    image: certbot/certbot:latest
    volumes:
      - ../nginx/ssl:/etc/letsencrypt
      - ../nginx/certbot/www:/var/www/certbot
      - ../logs/certbot:/var/log/letsencrypt
    command: certonly --webroot --webroot-path=/var/www/certbot --email ${SSL_EMAIL} --agree-tos --no-eff-email --force-renewal -d ${DOMAIN_NAME}
    profiles:
      - ssl-setup
    depends_on:
      - nginx
    restart: "no"

  # Nginx reverse proxy - Production optimized
  nginx:
    image: nginx:alpine
    environment:
      - DOMAIN_NAME=${DOMAIN_NAME}
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ../nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ../nginx/ssl:/etc/nginx/ssl:ro
      - ../nginx/ssl:/etc/letsencrypt:ro
      - ../nginx/certbot/www:/var/www/certbot:ro
      - ../logs/nginx:/var/log/nginx
      - ../nginx/conf.d/default.conf.template:/tmp/default.conf.template:ro
    depends_on:
      - ai
      - scraper
      - web
    restart: unless-stopped
    networks:
      - portfolio-network-prod
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:80/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: >
      /bin/sh -c "sed 's/__DOMAIN_NAME__/'\"$DOMAIN_NAME\"'/g' /tmp/default.conf.template > /tmp/default.conf && nginx -g 'daemon off;'"
    deploy:
      resources:
        limits:
          memory: ${NGINX_MEMORY_LIMIT:-256M}
          cpus: "${NGINX_CPU_LIMIT:-0.5}"
        reservations:
          memory: 128M
          cpus: "0.25"

  # Web Console - Scraper console
  web:
    image: ${WEB_IMAGE:-ghcr.io/your-username/personal-website/web:latest}
    environment:
      - NODE_ENV=${WEB_NODE_ENV:-production}
      - PORT=${WEB_PORT:-3000}
      # Server-side URLs (can access Docker internal network)
      - AI_URL=${AI_URL:-http://ai:3001}
      - SCRAPER_API_URL=${SCRAPER_API_URL:-http://scraper:8080}
      # Client-side URL (browser must go through nginx proxy)
      - NEXT_PUBLIC_SCRAPER_API_URL=/api/arachne
    depends_on:
      - ai
      - scraper
    restart: unless-stopped
    networks:
      - portfolio-network-prod
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:3000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: ${WEB_MEMORY_LIMIT:-512M}
          cpus: "${WEB_CPU_LIMIT:-1.0}"
        reservations:
          memory: 256M
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # AI Backend - Production optimized
  ai:
    image: ${AI_IMAGE:-ghcr.io/your-username/personal-website/ai:latest}
    environment:
      - NODE_ENV=${AI_NODE_ENV:-production}
      - PORT=${AI_PORT:-3001}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - LOG_LEVEL=${AI_LOG_LEVEL:-info}
      - ENABLE_METRICS=${AI_ENABLE_METRICS:-true}
      - RATE_LIMIT_WINDOW_MS=${AI_RATE_LIMIT_WINDOW_MS:-900000}
      - RATE_LIMIT_MAX_REQUESTS=${AI_RATE_LIMIT_MAX_REQUESTS:-100}
      - SESSION_TOKEN_SECRET=${SESSION_TOKEN_SECRET}
      - REDIS_URL=${AI_REDIS_URL:-redis://redis:6379/0}
      - TURNSTILE_SECRET=${TURNSTILE_SECRET}
      - TURNSTILE_REQUIRED=${TURNSTILE_REQUIRED:-false}
      - SCRAPER_URL=${SCRAPER_URL:-http://scraper:8080}
      - SCRAPER_API_TOKEN=${SCRAPER_API_TOKEN}
      - CORS_ALLOW_ORIGIN=${CORS_ALLOW_ORIGIN:-https://${DOMAIN_NAME}}
      - CORS_ALLOW_METHODS=${CORS_ALLOW_METHODS:-GET, POST, OPTIONS}
      - CORS_ALLOW_HEADERS=${CORS_ALLOW_HEADERS:-DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization}
      - PROFILE_CONTEXT_PATH=/app/profile/profile.md
    restart: unless-stopped
    networks:
      - portfolio-network-prod
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://127.0.0.1:3001/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: ${AI_MEMORY_LIMIT:-1G}
          cpus: "${AI_CPU_LIMIT:-1.0}"
        reservations:
          memory: 512M
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Scraper - Production optimized with enhanced settings
  scraper:
    image: ${SCRAPER_IMAGE:-ghcr.io/your-username/personal-website/scraper:latest}
    environment:
      - SCRAPER_REDIS_ADDR=${SCRAPER_REDIS_ADDR:-redis:6379}
      - SCRAPER_REDIS_DB=${SCRAPER_REDIS_DB:-0}
      - SCRAPER_ENABLE_METRICS=${SCRAPER_ENABLE_METRICS:-true}
      - SCRAPER_ENABLE_LOGGING=${SCRAPER_ENABLE_LOGGING:-true}
      - SCRAPER_LOG_LEVEL=${SCRAPER_LOG_LEVEL:-info}
      - SCRAPER_MAX_CONCURRENT=${SCRAPER_MAX_CONCURRENT:-10}
      - SCRAPER_REQUEST_TIMEOUT=${SCRAPER_REQUEST_TIMEOUT:-120s}
      - SCRAPER_TOTAL_TIMEOUT=${SCRAPER_TOTAL_TIMEOUT:-180s}
      - SCRAPER_USE_HEADLESS=${SCRAPER_USE_HEADLESS:-true}
      - SCRAPER_HEADLESS_IGNORE_CERT_ERRORS=${SCRAPER_HEADLESS_IGNORE_CERT_ERRORS:-false}
      - SCRAPER_HEADLESS_NO_SANDBOX=${SCRAPER_HEADLESS_NO_SANDBOX:-false}
      - SCRAPER_MAX_CONTENT_BYTES=${SCRAPER_MAX_CONTENT_BYTES:-400000}
      - SCRAPER_USER_AGENT=${SCRAPER_USER_AGENT:-Mozilla/5.0 (compatible; PortfolioBot/1.0)}
      - SCRAPER_RATE_LIMIT=${SCRAPER_RATE_LIMIT:-2}
      - SCRAPER_RATE_LIMIT_WINDOW=${SCRAPER_RATE_LIMIT_WINDOW:-1s}
      - SCRAPER_API_TOKEN=${SCRAPER_API_TOKEN}
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - portfolio-network-prod
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://127.0.0.1:8080/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: ${SCRAPER_MEMORY_LIMIT:-2G}
          cpus: "${SCRAPER_CPU_LIMIT:-1.0}"
        reservations:
          memory: 1G
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis - Production optimized
  redis:
    image: redis:7-alpine
    volumes:
      - redis_data_prod:/data
      - ../logs/redis:/var/log/redis
    command: >
      redis-server 
      --appendonly yes 
      --maxmemory ${REDIS_MAX_MEMORY:-512mb} 
      --maxmemory-policy ${REDIS_MAX_MEMORY_POLICY:-allkeys-lru}
      --save 900 1
      --save 300 10
      --save 60 10000
    restart: unless-stopped
    networks:
      - portfolio-network-prod
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_LIMIT:-1G}
          cpus: "${REDIS_CPU_LIMIT:-1.0}"
        reservations:
          memory: 512M
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis Commander - Optional, can be disabled in production
  redis-commander:
    image: rediscommander/redis-commander:latest
    environment:
      - REDIS_HOSTS=local:redis:${REDIS_PORT:-6379}
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - ${DOCKER_NETWORK_NAME:-portfolio-network}-prod
    profiles:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.25"

networks:
  portfolio-network-prod:
    external: true
    name: portfolio-network-prod

volumes:
  redis_data_prod:
    driver: local
